# -*- coding: utf-8 -*-
"""
é²æ£’çš„æ–¹æ¡ˆæçº²æŠ½å–æ™ºèƒ½ä½“ - ä¸‰å±‚åŒ¹é…ï¼ˆè§„åˆ™ + è¯­ä¹‰ + LLMåˆ¤åˆ«ï¼‰
ç›®æ ‡ï¼šä»ä»»æ„ä¸­æ–‡æŠ€æœ¯æ–‡æ¡£ä¸­ç¨³å®šæŠ½å–"å…«ã€æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡"ç»“æ„
"""

from typing import Dict, Any, List, Optional, Tuple
import re
import os
import pathlib
import datetime
from dataclasses import dataclass
from enum import Enum
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
from collections import Counter
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–jiebaåˆ†è¯
jieba.initialize()

# åŒä¹‰è¯è¯å…¸
SYNONYM_DICT = {
    "å°˜æºç‚¹æ•é›†ç½©": ["å¸é£ç½©", "é›†æ°”ç½©", "æ•é›†è£…ç½®", "æ”¶é›†ç½©", "é™¤å°˜ç½©"],
    "å¹³é¢ç®¡ç½‘è·¯ç”±": ["æ€»å›¾ç®¡çº¿", "ç®¡ç½‘å¸ƒç½®", "æµç¨‹ç®¡çº¿èµ°å‘", "ç®¡é“å¸ƒç½®", "ç®¡çº¿è·¯ç”±"],
    "é’¢æ¸£ä¸€æ¬¡å¤„ç†": ["é’¢æ¸£åˆçº§å¤„ç†", "é’¢æ¸£é¢„å¤„ç†", "ä¸€æ¬¡å¤„ç†å·¥è‰º", "åˆçº§å¤„ç†ç³»ç»Ÿ"],
    "é™¤å°˜ç³»ç»Ÿå¸ƒç½®": ["é™¤å°˜ç³»ç»Ÿå¸ƒå±€", "é™¤å°˜è®¾å¤‡å¸ƒç½®", "é™¤å°˜è£…ç½®å®‰æ’", "é™¤å°˜è®¾æ–½é…ç½®"],
    "å…³é”®æŠ€æœ¯è¯´æ˜": ["æ ¸å¿ƒæŠ€æœ¯è¯´æ˜", "æŠ€æœ¯è¦ç‚¹é˜è¿°", "å…³é”®æŠ€æœ¯é˜è¿°", "æŠ€æœ¯éš¾ç‚¹è¯´æ˜"],
    "æ–½å·¥æ–¹æ³•": ["æ–½å·¥å·¥è‰º", "æ–½å·¥æŠ€æœ¯", "æ–½å·¥æ–¹æ¡ˆ", "æ–½å·¥æµç¨‹"],
    "æŠ€æœ¯æªæ–½": ["æŠ€æœ¯æ–¹æ¡ˆ", "æŠ€æœ¯æ‰‹æ®µ", "æŠ€æœ¯æ–¹æ³•", "æŠ€æœ¯å¯¹ç­–"],
    "åœæœºæ—¶é—´": ["åœæœºé…åˆ", "åœäº§æ—¶é—´", "è®¾å¤‡åœæœº", "ç”Ÿäº§ä¸­æ–­"]
}

# ä¸­æ–‡æ•°å­—æ˜ å°„æ‰©å±•
CN_NUM_MAP = {
    'é›¶': 0, 'ã€‡': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
    'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10, 'åä¸€': 11, 'åäºŒ': 12,
    'åä¸‰': 13, 'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18,
    'åä¹': 19, 'äºŒå': 20, 'ä¸‰å': 30, 'å››å': 40, 'äº”å': 50, 'å…­å': 60,
    'ä¸ƒå': 70, 'å…«å': 80, 'ä¹å': 90, 'ç™¾': 100
}

try:
    from .base import BaseAgent, AgentContext, AgentResponse
except Exception:
    class BaseAgent:
        name = "base"
        def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
            return state
    
    class AgentContext:
        pass
    
    class AgentResponse:
        def __init__(self, content="", metadata=None):
            self.content = content
            self.metadata = metadata or {}


class MatchStatus(str, Enum):
    OK = "ok"
    MISSING = "missing"
    LOW_CONFIDENCE = "low_confidence"
    CONFLICT = "conflict"


@dataclass
class HeadingNode:
    level: int
    text: str
    normalized_text: str
    start_pos: int
    end_pos: int
    children: List['HeadingNode']
    content_start: Optional[int] = None
    content_end: Optional[int] = None


@dataclass
class TargetSection:
    key: str
    normalized_key: str
    description: str
    aliases: List[str]
    keywords: List[str]
    expected_level: int
    parent_key: Optional[str] = None


class OutlineExtractorAgent(BaseAgent):
    name = "outline_extractor"
    
    # ç›®æ ‡ç« èŠ‚å®šä¹‰ - å…«ã€æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡
    TARGET_SECTIONS = [
        # ä¸»ç« èŠ‚
        TargetSection(
            key="chapter.8.plan_and_org",
            normalized_key="å…«ã€æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡",
            description="ç¬¬å…«ç« æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡",
            aliases=[
                "æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡",
                "æŠ€æœ¯æ–¹æ¡ˆä¸æ–½å·¥ç»„ç»‡", 
                "å®æ–½æ–¹æ¡ˆä¸æ–½å·¥ç»„ç»‡",
                "æ–½å·¥ç»„ç»‡è®¾è®¡ä¸æŠ€æœ¯æ–¹æ¡ˆ",
                "å…«ã€", "ç¬¬å…«ç« ", "ç¬¬8ç« "
            ],
            keywords=["æ–¹æ¡ˆ", "è¯¦ç»†", "æ–½å·¥", "ç»„ç»‡", "è®¾è®¡", "æŠ€æœ¯", "æªæ–½"],
            expected_level=1
        ),
        
        # å­ç« èŠ‚ 1ã€æ–¹æ¡ˆçš„è¯¦ç»†è¯´æ˜
        TargetSection(
            key="plan.detailed_description",
            normalized_key="1ã€æ–¹æ¡ˆçš„è¯¦ç»†è¯´æ˜",
            description="ç¬¬ä¸€èŠ‚æ–¹æ¡ˆçš„è¯¦ç»†è¯´æ˜",
            aliases=["æ–¹æ¡ˆçš„è¯¦ç»†è¯´æ˜", "æŠ€æœ¯æ–¹æ¡ˆè¯´æ˜", "æ–¹æ¡ˆè¯´æ˜", "ä¸€ã€"],
            keywords=["æ–¹æ¡ˆ", "è¯¦ç»†", "è¯´æ˜", "æŠ€æœ¯"],
            expected_level=2,
            parent_key="chapter.8.plan_and_org"
        ),
        
        # å­ç« èŠ‚ 1.1 ä¼˜åŒ–æå‡æ”¹é€ éƒ¨åˆ†è¯¦ç»†æ–¹æ¡ˆè¯´æ˜
        TargetSection(
            key="plan.upgrade_detail",
            normalized_key="1.1 ä¼˜åŒ–æå‡æ”¹é€ éƒ¨åˆ†è¯¦ç»†æ–¹æ¡ˆè¯´æ˜",
            description="ä¼˜åŒ–æå‡æ”¹é€ éƒ¨åˆ†è¯¦ç»†æ–¹æ¡ˆè¯´æ˜",
            aliases=[
                "ä¼˜åŒ–æå‡æ”¹é€ éƒ¨åˆ†è¯¦ç»†æ–¹æ¡ˆè¯´æ˜",
                "æŠ€æœ¯æ”¹é€ æ–¹æ¡ˆè¯´æ˜", 
                "ä¼˜åŒ–æ”¹é€ è¯¦ç»†æ–¹æ¡ˆ",
                "æŠ€æ”¹æ–¹æ¡ˆ"
            ],
            keywords=["ä¼˜åŒ–", "æå‡", "æ”¹é€ ", "æŠ€æ”¹", "è¯¦ç»†", "æ–¹æ¡ˆ", "è¯´æ˜"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 1.2 å°˜æºç‚¹æ•é›†ç½©æ–¹æ¡ˆè¯¦ç»†è¯´æ˜
        TargetSection(
            key="plan.dust_capture",
            normalized_key="1.2 å°˜æºç‚¹æ•é›†ç½©æ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
            description="å°˜æºç‚¹æ•é›†ç½©æ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
            aliases=[
                "å°˜æºç‚¹æ•é›†ç½©æ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
                "å¸é£ç½©æ–¹æ¡ˆ", 
                "é›†æ°”ç½©æ–¹æ¡ˆ",
                "æ•é›†è£…ç½®æ–¹æ¡ˆ"
            ],
            keywords=["å°˜æº", "ç²‰å°˜", "æ•é›†", "æ”¶é›†", "ç½©", "å¸é£", "é›†æ°”"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 1.3 å¹³é¢ç®¡ç½‘è·¯ç”±æ–¹æ¡ˆå›¾åŠè¯´æ˜
        TargetSection(
            key="plan.pipeline_layout",
            normalized_key="1.3 å¹³é¢ç®¡ç½‘è·¯ç”±æ–¹æ¡ˆå›¾åŠè¯´æ˜",
            description="å¹³é¢ç®¡ç½‘è·¯ç”±æ–¹æ¡ˆå›¾åŠè¯´æ˜",
            aliases=[
                "å¹³é¢ç®¡ç½‘è·¯ç”±æ–¹æ¡ˆå›¾åŠè¯´æ˜",
                "æ€»å›¾ç®¡çº¿å¸ƒç½®", 
                "ç®¡ç½‘å¸ƒç½®æ–¹æ¡ˆ",
                "æµç¨‹ç®¡çº¿èµ°å‘"
            ],
            keywords=["å¹³é¢", "ç®¡ç½‘", "ç®¡çº¿", "ç®¡é“", "è·¯ç”±", "èµ°å‘", "å¸ƒç½®", "æ€»å›¾"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 1.4 é’¢æ¸£ä¸€æ¬¡å¤„ç†å·¥è‰ºç³»ç»Ÿæ–¹æ¡ˆå›¾åŠè¯¦ç»†è¯´æ˜
        TargetSection(
            key="plan.slag_system",
            normalized_key="1.4 é’¢æ¸£ä¸€æ¬¡å¤„ç†å·¥è‰ºç³»ç»Ÿæ–¹æ¡ˆå›¾åŠè¯¦ç»†è¯´æ˜",
            description="é’¢æ¸£ä¸€æ¬¡å¤„ç†å·¥è‰ºç³»ç»Ÿæ–¹æ¡ˆå›¾åŠè¯¦ç»†è¯´æ˜",
            aliases=[
                "é’¢æ¸£ä¸€æ¬¡å¤„ç†å·¥è‰ºç³»ç»Ÿæ–¹æ¡ˆå›¾åŠè¯¦ç»†è¯´æ˜",
                "é’¢æ¸£å¤„ç†ç³»ç»Ÿæ–¹æ¡ˆ", 
                "ä¸€æ¬¡å¤„ç†å·¥è‰ºæ–¹æ¡ˆ"
            ],
            keywords=["é’¢æ¸£", "ä¸€æ¬¡å¤„ç†", "åˆçº§å¤„ç†", "å·¥è‰ºç³»ç»Ÿ", "æµç¨‹", "æ–¹æ¡ˆ"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 1.5 é™¤å°˜ç³»ç»Ÿå¸ƒç½®å›¾åŠæ–¹æ¡ˆè¯¦ç»†è¯´æ˜
        TargetSection(
            key="plan.dedusting_layout",
            normalized_key="1.5 é™¤å°˜ç³»ç»Ÿå¸ƒç½®å›¾åŠæ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
            description="é™¤å°˜ç³»ç»Ÿå¸ƒç½®å›¾åŠæ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
            aliases=[
                "é™¤å°˜ç³»ç»Ÿå¸ƒç½®å›¾åŠæ–¹æ¡ˆè¯¦ç»†è¯´æ˜",
                "é™¤å°˜ç³»ç»Ÿå¸ƒå±€æ–¹æ¡ˆ", 
                "é™¤å°˜å¸ƒç½®å›¾"
            ],
            keywords=["é™¤å°˜", "é™¤å°˜ç³»ç»Ÿ", "å¸ƒç½®", "å¸ƒå±€", "æ–¹æ¡ˆ", "è¯´æ˜", "å›¾"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 1.6 å…³é”®æŠ€æœ¯è¯´æ˜ç­‰
        TargetSection(
            key="plan.key_tech",
            normalized_key="1.6 å…³é”®æŠ€æœ¯è¯´æ˜ç­‰",
            description="å…³é”®æŠ€æœ¯è¯´æ˜ç­‰",
            aliases=[
                "å…³é”®æŠ€æœ¯è¯´æ˜ç­‰",
                "æ ¸å¿ƒæŠ€æœ¯è¯´æ˜", 
                "æŠ€æœ¯è¦ç‚¹",
                "æŠ€æœ¯éš¾ç‚¹"
            ],
            keywords=["å…³é”®", "æ ¸å¿ƒ", "æŠ€æœ¯", "è¦ç‚¹", "éš¾ç‚¹", "è¯´æ˜"],
            expected_level=3,
            parent_key="plan.detailed_description"
        ),
        
        # å­ç« èŠ‚ 2ã€æ–½å·¥ç»„ç»‡è®¾è®¡
        TargetSection(
            key="org.construction_design",
            normalized_key="2ã€æ–½å·¥ç»„ç»‡è®¾è®¡",
            description="ç¬¬äºŒèŠ‚æ–½å·¥ç»„ç»‡è®¾è®¡",
            aliases=["æ–½å·¥ç»„ç»‡è®¾è®¡", "æ–½å·¥ç»„ç»‡", "äºŒã€"],
            keywords=["æ–½å·¥", "ç»„ç»‡", "è®¾è®¡"],
            expected_level=2,
            parent_key="chapter.8.plan_and_org"
        ),
        
        # å­ç« èŠ‚ 2.1 æ–½å·¥æ–¹æ³•åŠä¸»è¦æŠ€æœ¯æªæ–½ï¼ˆæ–½å·¥æ–¹æ¡ˆï¼‰
        TargetSection(
            key="org.construction_method",
            normalized_key="2.1 æ–½å·¥æ–¹æ³•åŠä¸»è¦æŠ€æœ¯æªæ–½ï¼ˆæ–½å·¥æ–¹æ¡ˆï¼‰",
            description="æ–½å·¥æ–¹æ³•åŠä¸»è¦æŠ€æœ¯æªæ–½ï¼ˆæ–½å·¥æ–¹æ¡ˆï¼‰",
            aliases=[
                "æ–½å·¥æ–¹æ³•åŠä¸»è¦æŠ€æœ¯æªæ–½ï¼ˆæ–½å·¥æ–¹æ¡ˆï¼‰",
                "æ–½å·¥æ–¹æ³•ä¸æŠ€æœ¯æªæ–½", 
                "æ–½å·¥æ–¹æ¡ˆ"
            ],
            keywords=["æ–½å·¥", "æ–¹æ³•", "æŠ€æœ¯", "æªæ–½", "æ–¹æ¡ˆ"],
            expected_level=3,
            parent_key="org.construction_design"
        ),
        
        # å­ç« èŠ‚ 2.2 éœ€æŠ•æ ‡äººé…åˆåœæœºæ—¶é—´çš„è¯¦ç»†ç»„ç»‡è®¾è®¡
        TargetSection(
            key="org.shutdown_coordination",
            normalized_key="2.2 éœ€æŠ•æ ‡äººé…åˆåœæœºæ—¶é—´çš„è¯¦ç»†ç»„ç»‡è®¾è®¡",
            description="éœ€æŠ•æ ‡äººé…åˆåœæœºæ—¶é—´çš„è¯¦ç»†ç»„ç»‡è®¾è®¡",
            aliases=[
                "éœ€æŠ•æ ‡äººé…åˆåœæœºæ—¶é—´çš„è¯¦ç»†ç»„ç»‡è®¾è®¡",
                "åœæœºæ—¶é—´é…åˆç»„ç»‡", 
                "åœæœºç»„ç»‡å®‰æ’"
            ],
            keywords=["åœæœº", "é…åˆ", "æ—¶é—´", "ç»„ç»‡", "å®‰æ’", "è®¡åˆ’"],
            expected_level=3,
            parent_key="org.construction_design"
        )
    ]
    
    def __init__(self):
        try:
            super().__init__(self.name)
        except:
            # å¦‚æœLLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åˆå§‹åŒ–
            self.llm_client = None
        self.vectorizer = TfidfVectorizer()
        
    def get_system_prompt(self) -> str:
        return "ä½ æ˜¯æ–¹æ¡ˆç»“æ„æŠ½å–ä¸“å®¶ï¼Œä»æŠ€æœ¯æ–‡æ¡£ä¸­ç²¾ç¡®è¯†åˆ«å’ŒæŠ½å–æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡çš„ç« èŠ‚ç»“æ„ã€‚"
    
    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ–¹æ¡ˆç»“æ„æŠ½å–ï¼ˆå·¥ä½œæµå…¼å®¹ç‰ˆæœ¬ï¼‰"""
        try:
            # å°†å­—å…¸çŠ¶æ€è½¬æ¢ä¸ºAgentContext
            context = self._state_to_context(state)
            
            # è·å–æ‹›æ ‡æ–‡ä»¶å†…å®¹
            tender_text = self._extract_tender_text(context)
            if not tender_text:
                # è¿”å›é”™è¯¯çŠ¶æ€
                state["error"] = "æœªæ‰¾åˆ°æ‹›æ ‡æ–‡ä»¶å†…å®¹"
                state["outline_extraction_status"] = "failed"
                return state
            
            # æ„å»ºæ ‡é¢˜æ ‘
            heading_tree = self._build_heading_tree(tender_text)
            
            # å®šä½ç¬¬å…«ç« 
            chapter_8 = self._locate_chapter_8(heading_tree, tender_text)
            
            # æŠ½å–å­ç« èŠ‚ç»“æ„
            results = self._extract_sub_sections(chapter_8, tender_text)
            
            # å°†ç»“æœå­˜å…¥çŠ¶æ€
            state["outline_results"] = results
            state["outline_extraction_status"] = "success"
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            matched_ok = sum(1 for r in results if r['status'] == MatchStatus.OK)
            matched_low = sum(1 for r in results if r['status'] == MatchStatus.LOW_CONFIDENCE)
            matched_missing = sum(1 for r in results if r['status'] == MatchStatus.MISSING)
            total = len(results)
            
            state["outline_matched_ok"] = matched_ok
            state["outline_matched_low"] = matched_low
            state["outline_missing"] = matched_missing
            state["outline_total"] = total
            state["outline_confidence"] = sum(r['confidence'] for r in results) / total if total > 0 else 0.0
            
            return state
            
        except Exception as e:
            state["error"] = f"ç»“æ„æŠ½å–å¤±è´¥: {str(e)}"
            state["outline_extraction_status"] = "failed"
            return state
    
    def _state_to_context(self, state: Dict[str, Any]) -> AgentContext:
        """å°†å­—å…¸çŠ¶æ€è½¬æ¢ä¸ºAgentContext"""
        try:
            from .base import AgentContext
            return AgentContext(
                user_input=state.get("user_input", "æŠ½å–æ–¹æ¡ˆç»“æ„"),
                uploaded_files=state.get("uploaded_files", []),
                parsed_documents=state.get("parsed_documents", []),
                project_state=state.get("project_state", {})
            )
        except:
            # å¤‡ç”¨å®ç°
            class SimpleContext:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        setattr(self, k, v)
            return SimpleContext(
                user_input=state.get("user_input", "æŠ½å–æ–¹æ¡ˆç»“æ„"),
                uploaded_files=state.get("uploaded_files", []),
                parsed_documents=state.get("parsed_documents", []),
                project_state=state.get("project_state", {})
            )

    def _extract_tender_text(self, context) -> Optional[str]:
        """ä»ä¸Šä¸‹æ–‡ä¸­æå–æ‹›æ ‡æ–‡ä»¶æ–‡æœ¬"""
        # å°è¯•ä»ä¸åŒæ¥æºè·å–æ–‡æœ¬
        project_state = getattr(context, 'project_state', {})
        
        # 1. ä»ä¸Šä¼ æ–‡ä»¶è·å–
        uploaded_files = getattr(context, 'uploaded_files', [])
        for file_info in uploaded_files:
            if file_info.get('filename', '').endswith('.md'):
                file_path = file_info.get('path')
                if file_path and os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return f.read()
        
        # 2. ä»é¡¹ç›®çŠ¶æ€è·å–
        tender_path = project_state.get('tender_path')
        if tender_path and os.path.exists(tender_path):
            with open(tender_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        # 3. ä»è§£æçš„æ–‡æ¡£è·å–
        parsed_docs = getattr(context, 'parsed_documents', [])
        for doc in parsed_docs:
            if doc.get('filename', '').endswith('.md'):
                content = doc.get('content')
                if content:
                    return content
        
        return None
    
    def _build_heading_tree(self, text: str) -> List[HeadingNode]:
        """æ„å»ºæ–‡æ¡£æ ‡é¢˜æ ‘"""
        lines = text.split('\n')
        headings = []
        stack = []
        
        for i, line in enumerate(lines):
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            level, title = self._parse_heading_line(line)
            if level > 0:
                node = HeadingNode(
                    level=level,
                    text=line.strip(),
                    normalized_text=self._normalize_heading(title),
                    start_pos=text.find(line),
                    end_pos=text.find(line) + len(line),
                    children=[]
                )
                
                # æ„å»ºå±‚çº§å…³ç³»
                while stack and stack[-1].level >= level:
                    stack.pop()
                
                if stack:
                    stack[-1].children.append(node)
                else:
                    headings.append(node)
                
                stack.append(node)
        
        return headings
    
    def _parse_heading_line(self, line: str) -> Tuple[int, str]:
        """è§£ææ ‡é¢˜è¡Œï¼Œè¿”å›å±‚çº§å’Œæ ‡é¢˜æ–‡æœ¬"""
        line = line.strip()
        
        # åŒ¹é… Markdown æ ‡é¢˜
        md_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if md_match:
            level = len(md_match.group(1))
            title = md_match.group(2)
            return level, title
        
        # åŒ¹é…ä¸­æ–‡æ•°å­—æ ‡é¢˜
        cn_match = re.match(r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€.:]\s*(.+)$', line)
        if cn_match:
            level = self._chinese_number_to_int(cn_match.group(1))
            title = cn_match.group(2)
            return min(level, 6), title  # é™åˆ¶æœ€å¤§å±‚çº§
        
        # åŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—æ ‡é¢˜
        num_match = re.match(r'^(\d+)[ã€.:]\s*(.+)$', line)
        if num_match:
            level = int(num_match.group(1))
            title = num_match.group(2)
            return min(level, 6), title
        
        return 0, ""
    
    def _chinese_number_to_int(self, cn_num: str) -> int:
        """ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆæ”¯æŒå¤æ‚æ•°å­—ï¼‰"""
        if cn_num.isdigit():
            return int(cn_num)
        
        # å¤„ç†å¤æ‚ä¸­æ–‡æ•°å­—
        total = 0
        current = 0
        
        for char in cn_num:
            if char in CN_NUM_MAP:
                value = CN_NUM_MAP[char]
                if value >= 10:  # åã€ç™¾ç­‰
                    if current == 0:
                        current = 1
                    total += current * value
                    current = 0
                else:
                    current = value
            else:
                # éæ•°å­—å­—ç¬¦ï¼Œè·³è¿‡
                continue
        
        total += current
        return total if total > 0 else 0
    
    def _normalize_heading(self, text: str) -> str:
        """å¢å¼ºçš„æ ‡é¢˜å½’ä¸€åŒ–"""
        if not text:
            return ""
        
        # 1. ç»Ÿä¸€ç¼–å·æ ¼å¼
        text = re.sub(r'^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡0-9]+)\s*ç« \s*[:ï¼š]?', r'\1ã€', text)
        text = re.sub(r'^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡0-9]+)\s*èŠ‚\s*[:ï¼š]?', r'\1ã€', text)
        
        # 2. ç»Ÿä¸€åˆ†éš”ç¬¦
        text = re.sub(r'[ã€.:ï¼ã€‚]', '.', text)
        
        # 3. ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—
        def replace_chinese_numbers(match):
            cn_num = match.group(1)
            return str(self._chinese_number_to_int(cn_num))
        
        text = re.sub(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡]+)', replace_chinese_numbers, text)
        
        # 4. å»é™¤æ‹¬å·å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[ï¼ˆï¼‰()\[\]ã€ã€‘ã€Œã€ã€Šã€‹<>\*\-\+\s]', '', text)
        
        # 5. è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # 6. æ ‡å‡†åŒ–ç¼–å·æ ¼å¼
        text = re.sub(r'(\d+)\.(\d+)', r'\1.\2', text)  # ç¡®ä¿ç‚¹å·æ ¼å¼
        
        return text.strip()
    
    def _locate_chapter_8(self, heading_tree: List[HeadingNode], text: str) -> Optional[HeadingNode]:
        """å®šä½ç¬¬å…«ç« """
        # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
        for node in heading_tree:
            if self._match_chapter_8(node):
                return node
        
        # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨è¯­ä¹‰æœç´¢
        return self._semantic_locate_chapter_8(text)
    
    def _match_chapter_8(self, node: HeadingNode) -> bool:
        """åŒ¹é…ç¬¬å…«ç« """
        target = self.TARGET_SECTIONS[0]  # ç¬¬å…«ç« 
        
        # è§„åˆ™å±‚åŒ¹é…
        rule_score = self._rule_layer_score(node.normalized_text, target)
        # è¯­ä¹‰å±‚åŒ¹é…
        semantic_score = self._semantic_layer_score(node.text, target)
        
        # èåˆå¾—åˆ†
        total_score = 0.4 * rule_score + 0.6 * semantic_score
        return total_score >= 0.7
    
    def _rule_layer_score(self, text: str, target: TargetSection) -> float:
        """å¢å¼ºçš„è§„åˆ™å±‚åŒ¹é…å¾—åˆ†"""
        score = 0.0
        
        # 1. ç²¾ç¡®åˆ«ååŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
        for alias in target.aliases:
            if alias in text:
                score += 0.4
                logger.debug(f"ç²¾ç¡®åˆ«ååŒ¹é…: {alias} -> +0.4")
                break
        
        # 2. å…³é”®è¯è¦†ç›–ç‡
        matched_keywords = []
        for keyword in target.keywords:
            if keyword in text:
                score += 0.08  # é™ä½å•ä¸ªå…³é”®è¯æƒé‡
                matched_keywords.append(keyword)
        
        if matched_keywords:
            logger.debug(f"å…³é”®è¯åŒ¹é…: {matched_keywords} -> +{0.08 * len(matched_keywords):.2f}")
        
        # 3. åŒä¹‰è¯æ‰©å±•åŒ¹é…
        synonym_score = self._synonym_match_score(text, target)
        score += synonym_score * 0.2
        
        # 4. ç¼–å·æ ¼å¼åŒ¹é…
        numbering_score = self._numbering_format_score(text, target)
        score += numbering_score * 0.1
        
        return min(score, 1.0)
    
    def _synonym_match_score(self, text: str, target: TargetSection) -> float:
        """åŒä¹‰è¯åŒ¹é…å¾—åˆ†"""
        score = 0.0
        
        # æ£€æŸ¥ç›®æ ‡æè¿°ä¸­çš„å…³é”®è¯æ˜¯å¦åœ¨åŒä¹‰è¯è¯å…¸ä¸­
        for keyword in target.keywords:
            if keyword in SYNONYM_DICT:
                synonyms = SYNONYM_DICT[keyword]
                for synonym in synonyms:
                    if synonym in text:
                        score += 0.2
                        logger.debug(f"åŒä¹‰è¯åŒ¹é…: {keyword} -> {synonym} -> +0.2")
                        break
        
        return min(score, 1.0)
    
    def _numbering_format_score(self, text: str, target: TargetSection) -> float:
        """ç¼–å·æ ¼å¼åŒ¹é…å¾—åˆ†"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡å‡†ç¼–å·æ ¼å¼
        if re.search(r'\d+(\.\d+)*', text):
            return 0.3
        
        # æ£€æŸ¥ä¸­æ–‡ç¼–å·æ ¼å¼
        if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+(?:[ã€.][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)*', text):
            return 0.2
        
        return 0.0
    
    def _semantic_layer_score(self, text: str, target: TargetSection) -> float:
        """å¢å¼ºçš„è¯­ä¹‰å±‚åŒ¹é…å¾—åˆ†"""
        score = 0.0
        
        # 1. TF-IDF ä½™å¼¦ç›¸ä¼¼åº¦
        try:
            corpus = [text, target.description] + target.aliases
            tfidf_matrix = self.vectorizer.fit_transform(corpus)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            score += similarity * 0.5
            logger.debug(f"TF-IDFç›¸ä¼¼åº¦: {similarity:.3f} -> +{similarity * 0.5:.3f}")
        except Exception as e:
            logger.warning(f"TF-IDFè®¡ç®—å¤±è´¥: {e}")
        
        # 2. Jaccardç›¸ä¼¼åº¦ï¼ˆåˆ†è¯çº§åˆ«ï¼‰
        jaccard_score = self._jaccard_similarity(text, target.description)
        score += jaccard_score * 0.3
        logger.debug(f"Jaccardç›¸ä¼¼åº¦: {jaccard_score:.3f} -> +{jaccard_score * 0.3:.3f}")
        
        # 3. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        edit_distance_score = self._edit_distance_similarity(text, target.description)
        score += edit_distance_score * 0.2
        logger.debug(f"ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦: {edit_distance_score:.3f} -> +{edit_distance_score * 0.2:.3f}")
        
        return min(score, 1.0)
    
    def _jaccard_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—Jaccardç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # åˆ†è¯
        words1 = set(jieba.cut(text1))
        words2 = set(jieba.cut(text2))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _edit_distance_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        len1, len2 = len(text1), len(text2)
        max_len = max(len1, len2)
        
        if max_len == 0:
            return 1.0
        
        # è®¡ç®—å…¬å…±å­ä¸²é•¿åº¦
        common = 0
        for i in range(min(len1, len2)):
            if text1[i] == text2[i]:
                common += 1
            else:
                break
        
        return common / max_len
    
    def _semantic_locate_chapter_8(self, text: str) -> Optional[HeadingNode]:
        """å¢å¼ºçš„è¯­ä¹‰å®šä½ç¬¬å…«ç« """
        lines = text.split('\n')
        
        # æ–¹æ¡ˆç›¸å…³å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰
        keyword_weights = {
            "æ–¹æ¡ˆ": 2.0, "æ–½å·¥": 1.5, "ç»„ç»‡": 1.5, "è®¾è®¡": 1.5, 
            "æŠ€æœ¯": 1.2, "æªæ–½": 1.2, "è¯¦ç»†": 1.0, "è¯´æ˜": 1.0,
            "ä¼˜åŒ–": 1.0, "æ”¹é€ ": 1.0, "å°˜æº": 0.8, "æ•é›†": 0.8,
            "ç®¡ç½‘": 0.8, "è·¯ç”±": 0.8, "é’¢æ¸£": 0.8, "é™¤å°˜": 0.8,
            "å…³é”®": 0.8, "åœæœº": 0.8, "é…åˆ": 0.8, "æ—¶é—´": 0.5
        }
        
        best_score = 0.0
        best_start = -1
        best_end = -1
        
        # æ»‘åŠ¨çª—å£æ£€æµ‹ï¼ˆæ›´å¤§çš„çª—å£ï¼‰
        window_size = 15
        for i in range(len(lines) - window_size + 1):
            window_lines = lines[i:i+window_size]
            window_text = ' '.join(window_lines)
            
            # è®¡ç®—åŠ æƒå…³é”®è¯å¯†åº¦
            window_score = 0.0
            keyword_count = 0
            
            for keyword, weight in keyword_weights.items():
                if keyword in window_text:
                    window_score += weight
                    keyword_count += 1
            
            # å½’ä¸€åŒ–å¾—åˆ†
            if keyword_count > 0:
                window_score = window_score / sum(keyword_weights.values()) * 2
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¬¬å…«ç« çš„å…¸å‹æ¨¡å¼
            pattern_bonus = 0.0
            if any(pattern in window_text for pattern in ["ç¬¬å…«ç« ", "ç¬¬8ç« ", "å…«ã€"]):
                pattern_bonus += 0.3
            
            if any(pattern in window_text for pattern in ["æ–¹æ¡ˆè¯¦ç»†", "æ–½å·¥ç»„ç»‡", "æŠ€æœ¯æ–¹æ¡ˆ"]):
                pattern_bonus += 0.2
            
            total_score = window_score + pattern_bonus
            
            if total_score > best_score:
                best_score = total_score
                best_start = i
                best_end = i + window_size - 1
        
        if best_score >= 0.4:  # é™ä½é˜ˆå€¼ä»¥æé«˜å¬å›ç‡
            start_pos = text.find(lines[best_start])
            end_pos = text.find(lines[best_end]) + len(lines[best_end])
            
            # åˆ›å»ºä¼ªç¬¬å…«ç« èŠ‚ç‚¹
            return HeadingNode(
                level=1,
                text="ç¬¬å…«ç«  æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡ï¼ˆè¯­ä¹‰å®šä½ï¼‰",
                normalized_text="8ã€æ–¹æ¡ˆè¯¦ç»†è¯´æ˜åŠæ–½å·¥ç»„ç»‡è®¾è®¡",
                start_pos=start_pos,
                end_pos=end_pos,
                children=[]
            )
        
        logger.warning("è¯­ä¹‰å®šä½ç¬¬å…«ç« å¤±è´¥ï¼Œæœªæ‰¾åˆ°æ–¹æ¡ˆç›¸å…³å¯†é›†åŒºåŸŸ")
        return None
    
    def _extract_sub_sections(self, chapter_node: Optional[HeadingNode], text: str) -> List[Dict]:
        """å¢å¼ºçš„å­ç« èŠ‚ç»“æ„æŠ½å–"""
        if not chapter_node:
            # å¦‚æœæ‰¾ä¸åˆ°ç¬¬å…«ç« ï¼Œè¿”å›æ¨¡æ¿ç»“æ„
            return self._create_template_structure()
        
        results = []
        
        # éå†æ‰€æœ‰ç›®æ ‡å­ç« èŠ‚
        for target in self.TARGET_SECTIONS[1:]:  # è·³è¿‡ç¬¬å…«ç« 
            best_match = None
            best_score = 0.0
            alternatives = []
            
            # åœ¨ç¬¬å…«ç« çš„å­èŠ‚ç‚¹ä¸­æœç´¢åŒ¹é…
            if chapter_node.children:
                for child in chapter_node.children:
                    score = self._match_sub_section(child, target)
                    
                    # è®°å½•æ‰€æœ‰å€™é€‰
                    if score > 0.3:
                        alternatives.append({
                            'node': child,
                            'score': score,
                            'text': child.text
                        })
                    
                    if score > best_score:
                        best_score = score
                        best_match = child
            
            # å¦‚æœæ²¡æœ‰åœ¨ç›´æ¥å­èŠ‚ç‚¹ä¸­æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ•´ä¸ªç¬¬å…«ç« èŒƒå›´å†…æœç´¢
            if best_score < 0.5:
                best_match, best_score = self._search_in_chapter_content(chapter_node, text, target)
            
            if best_match and best_score >= 0.5:  # é™ä½é˜ˆå€¼
                # æå–å†…å®¹é¢„è§ˆ
                content_preview = self._extract_content_preview(text, best_match)
                
                results.append({
                    'normalized_key': target.normalized_key,
                    'found_title': best_match.text,
                    'location': {'start': best_match.start_pos, 'end': best_match.end_pos},
                    'confidence': best_score,
                    'content_preview': content_preview,
                    'status': MatchStatus.OK if best_score >= 0.6 else MatchStatus.LOW_CONFIDENCE,
                    'alternatives': [alt for alt in alternatives if alt['score'] >= 0.4]
                })
            else:
                results.append({
                    'normalized_key': target.normalized_key,
                    'found_title': '',
                    'location': None,
                    'confidence': best_score,
                    'content_preview': '',
                    'status': MatchStatus.MISSING,
                    'alternatives': alternatives
                })
        
        return results
    
    def _search_in_chapter_content(self, chapter_node: HeadingNode, text: str, target: TargetSection) -> Tuple[Optional[HeadingNode], float]:
        """åœ¨æ•´ä¸ªç¬¬å…«ç« å†…å®¹èŒƒå›´å†…æœç´¢åŒ¹é…çš„å­ç« èŠ‚"""
        # è·å–ç¬¬å…«ç« å†…å®¹èŒƒå›´
        chapter_content = text[chapter_node.start_pos:chapter_node.end_pos]
        
        # åœ¨å†…å®¹ä¸­æœç´¢ç›®æ ‡å…³é”®è¯
        best_score = 0.0
        best_match = None
        
        # æœç´¢åŒ…å«ç›®æ ‡å…³é”®è¯çš„æ®µè½
        paragraphs = chapter_content.split('\n\n')
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # æ£€æŸ¥æ®µè½æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
            score = self._rule_layer_score(para, target) * 0.6 + self._semantic_layer_score(para, target) * 0.4
            
            if score > best_score:
                best_score = score
                # åˆ›å»ºä¼ªèŠ‚ç‚¹
                best_match = HeadingNode(
                    level=target.expected_level,
                    text=para[:100] + "..." if len(para) > 100 else para,
                    normalized_text=self._normalize_heading(para[:50]),
                    start_pos=text.find(para),
                    end_pos=text.find(para) + len(para),
                    children=[]
                )
        
        return best_match, best_score
    
    def _extract_content_preview(self, text: str, node: HeadingNode, max_length: int = 200) -> str:
        """æå–å†…å®¹é¢„è§ˆ"""
        if node.content_start and node.content_end:
            content = text[node.content_start:node.content_end]
        else:
            # ä»èŠ‚ç‚¹ä½ç½®å‘åæå–ä¸€å®šé•¿åº¦çš„å†…å®¹
            content_start = node.end_pos
            content_end = min(len(text), content_start + 500)  # æå–500å­—ç¬¦
            content = text[content_start:content_end]
        
        # æ¸…ç†å†…å®¹
        content = re.sub(r'\s+', ' ', content.strip())
        
        # æˆªå–é¢„è§ˆ
        if len(content) > max_length:
            return content[:max_length] + "..."
        return content
    
    def _match_sub_section(self, node: HeadingNode, target: TargetSection) -> float:
        """å¢å¼ºçš„å­ç« èŠ‚åŒ¹é…"""
        # è§„åˆ™å±‚å¾—åˆ†
        rule_score = self._rule_layer_score(node.normalized_text, target)
        
        # è¯­ä¹‰å±‚å¾—åˆ†
        semantic_score = self._semantic_layer_score(node.text, target)
        
        # å±‚çº§åŒ¹é…å¾—åˆ†
        level_diff = abs(node.level - target.expected_level)
        level_score = max(0.1, 1.0 - level_diff * 0.3)  # å±‚çº§å·®å¼‚æƒ©ç½š
        
        # ä½ç½®å¾—åˆ†ï¼ˆåœ¨ç¬¬å…«ç« ä¸­çš„ç›¸å¯¹ä½ç½®ï¼‰
        position_score = self._calculate_position_score(node, target)
        
        # èåˆå¾—åˆ†ï¼ˆè°ƒæ•´æƒé‡ï¼‰
        total_score = (
            0.35 * rule_score + 
            0.35 * semantic_score + 
            0.2 * level_score + 
            0.1 * position_score
        )
        
        logger.debug(f"åŒ¹é…å­ç« èŠ‚: {node.text} -> {target.normalized_key}")
        logger.debug(f"  è§„åˆ™åˆ†: {rule_score:.3f}, è¯­ä¹‰åˆ†: {semantic_score:.3f}, å±‚çº§åˆ†: {level_score:.3f}, ä½ç½®åˆ†: {position_score:.3f}")
        logger.debug(f"  æ€»åˆ†: {total_score:.3f}")
        
        return total_score
    
    def _calculate_position_score(self, node: HeadingNode, target: TargetSection) -> float:
        """è®¡ç®—ä½ç½®å¾—åˆ†ï¼ˆåŸºäºç›®æ ‡ç« èŠ‚çš„é¢„æœŸä½ç½®ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ç›®æ ‡ç« èŠ‚çš„é¢„æœŸé¡ºåºç»™å‡ºä½ç½®å¥–åŠ±
        # ä¾‹å¦‚ï¼š1.1 åº”è¯¥åœ¨ 1.2 ä¹‹å‰ç­‰
        
        # ç®€å•å®ç°ï¼šç»™æ‰€æœ‰èŠ‚ç‚¹åŸºç¡€åˆ†
        return 0.5
    
    def _create_template_structure(self) -> List[Dict]:
        """åˆ›å»ºæ¨¡æ¿ç»“æ„"""
        return [
            {
                'normalized_key': target.normalized_key,
                'found_title': '',
                'location': None,
                'confidence': 0.0,
                'status': MatchStatus.MISSING
            }
            for target in self.TARGET_SECTIONS[1:]  # è·³è¿‡ç¬¬å…«ç« 
        ]
    
    def _create_success_response(self, results: List[Dict], text: str) -> Dict[str, Any]:
        """åˆ›å»ºå¢å¼ºçš„æˆåŠŸå“åº”"""
        # ç»Ÿè®¡åŒ¹é…æƒ…å†µ
        matched_ok = sum(1 for r in results if r['status'] == MatchStatus.OK)
        matched_low = sum(1 for r in results if r['status'] == MatchStatus.LOW_CONFIDENCE)
        matched_missing = sum(1 for r in results if r['status'] == MatchStatus.MISSING)
        total = len(results)
        
        content = f"âœ… æ–¹æ¡ˆç»“æ„æŠ½å–å®Œæˆ ({matched_ok}ä¸ªç²¾ç¡®åŒ¹é…, {matched_low}ä¸ªä½ç½®ä¿¡åº¦, {matched_missing}ä¸ªç¼ºå¤±)\n\n"
        
        for result in results:
            if result['status'] == MatchStatus.OK:
                status_icon = "âœ…"
                status_text = "ç²¾ç¡®åŒ¹é…"
            elif result['status'] == MatchStatus.LOW_CONFIDENCE:
                status_icon = "âš ï¸"
                status_text = "ä½ç½®ä¿¡åº¦"
            else:
                status_icon = "âŒ"
                status_text = "ç¼ºå¤±"
            
            content += f"{status_icon} {result['normalized_key']} ({status_text})"
            
            if result['found_title']:
                content += f" â†’ {result['found_title']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})"
            
            if result.get('content_preview'):
                content += f"\n   ğŸ“ å†…å®¹é¢„è§ˆ: {result['content_preview']}"
            
            if result.get('alternatives') and len(result['alternatives']) > 0:
                content += f"\n   ğŸ” å¤‡é€‰åŒ¹é…: {len(result['alternatives'])}ä¸ª"
            
            content += "\n\n"
        
        # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
        content += f"\nğŸ“Š åŒ¹é…ç»Ÿè®¡:\n"
        content += f"   â€¢ ç²¾ç¡®åŒ¹é…: {matched_ok}/{total}\n"
        content += f"   â€¢ ä½ç½®ä¿¡åº¦: {matched_low}/{total}\n"
        content += f"   â€¢ ç¼ºå¤±ç« èŠ‚: {matched_missing}/{total}\n"
        content += f"   â€¢ æ€»ä½“ç½®ä¿¡åº¦: {sum(r['confidence'] for r in results) / total:.2f}\n"
        
        return {
            'content': content,
            'metadata': {
                'outline_results': results,
                'matched_ok_count': matched_ok,
                'matched_low_count': matched_low,
                'missing_count': matched_missing,
                'total_count': total,
                'overall_confidence': sum(r['confidence'] for r in results) / total,
                'action': 'outline_extracted',
                'timestamp': datetime.datetime.now().isoformat()
            }
        }
    
    def _create_error_response(self, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            'content': f"âŒ {error_msg}",
            'metadata': {'action': 'outline_extraction_failed', 'error': error_msg},
            'status': 'error'
        }