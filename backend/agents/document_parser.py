"""
æ–‡æ¡£è§£ææ™ºèƒ½ä½“ - è´Ÿè´£è§£æä¸Šä¼ çš„æ‹›æ ‡æ–‡ä»¶å¹¶ç”Ÿæˆç»“æ„åŒ–çš„Markdownæ–‡æ¡£
"""
from typing import Dict, Any, List
import os
import json
import binascii
import logging
from pathlib import Path
from .base import BaseAgent, AgentContext, AgentResponse


class DocumentParserAgent(BaseAgent):
    def __init__(self):
        super().__init__("document_parser")
        self.logger = logging.getLogger(__name__)
    
    def get_system_prompt(self) -> str:
        return """æ‚¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è§£ææ™ºèƒ½ä½“ï¼Œä¸“é—¨è´Ÿè´£è§£ææ‹›æ ‡æ–‡ä»¶ï¼ˆPDF/DOC/DOCXï¼‰å¹¶è¯†åˆ«æ–‡æ¡£ç»“æ„ã€‚

æ‚¨çš„ä¸»è¦èŒè´£ï¼š
1. è¯»å–ä¸Šä¼ çš„æ‹›æ ‡æ–‡ä»¶
2. è¯†åˆ«æ–‡æ¡£ç»“æ„ï¼ˆç« èŠ‚æ ‡é¢˜ã€å†…å®¹ã€é¡µç ç­‰ï¼‰
3. æå–æ–‡æ¡£å…ƒæ•°æ®
4. å°†è§£æç»“æœä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹ä¸­

è§£æè¦æ±‚ï¼š
- å‡†ç¡®è¯†åˆ«ç« èŠ‚æ ‡é¢˜å’Œå±‚çº§å…³ç³»
- æå–æ–‡æ¡£ä¸­çš„è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æ„åŒ–ä¿¡æ¯
- ä¿æŒåŸæ–‡æ¡£çš„é€»è¾‘ç»“æ„
- ç”Ÿæˆç»“æ„åŒ–çš„JSONæ ¼å¼è¾“å‡º

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
{
  "filename": "åŸæ–‡ä»¶å",
  "document_type": "æ‹›æ ‡æ–‡ä»¶ç±»å‹",
  "structure": [
    {
      "level": 1,
      "title": "ç« èŠ‚æ ‡é¢˜",
      "content": "ç« èŠ‚å†…å®¹",
      "page": "é¡µç ",
      "subsections": []
    }
  ],
  "metadata": {
    "total_pages": "æ€»é¡µæ•°",
    "creation_date": "åˆ›å»ºæ—¥æœŸ",
    "file_size": "æ–‡ä»¶å¤§å°"
  }
}"""
    
    async def execute(self, context: AgentContext) -> AgentResponse:
        try:
            parsed_documents = []
            files_to_create: List[Dict[str, Any]] = []
            aggregated_extracted: Dict[str, Any] = {}
            
            for file_info in context.uploaded_files:
                filename = file_info.get('name', 'unknown')
                file_content = file_info.get('content', '')
                file_type = file_info.get('type', '')
                self.logger.info(f"[Upload] Received file: name={filename}, type={file_type}, content_len={len(file_content)}")
                
                if not file_content:
                    continue
                
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°uploadsæ–‡ä»¶å¤¹
                uploads_dir = "/root/project/git/project-agent/uploads"
                os.makedirs(uploads_dir, exist_ok=True)
                # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
                safe_filename = ''.join(c for c in filename if c.isalnum() or c in (' ', '.', '-', '_')).rstrip()
                upload_path = os.path.join(uploads_dir, safe_filename)
                
                # å¤„ç†æ–‡ä»¶å†…å®¹ - æ”¯æŒbase64ç¼–ç å’Œçº¯æ–‡æœ¬
                import base64
                try:
                    # é¦–å…ˆå°è¯•è§£ç base64
                    try:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºshellå‘½ä»¤è€ŒéçœŸå®base64å†…å®¹
                        if file_content.startswith('$(') and file_content.endswith(')'):
                            print(f"Warning: File content appears to be a shell command, not base64: {file_content[:50]}...")
                            raise ValueError("File content is a shell command, not actual file content")
                        
                        # ç¡®ä¿base64å­—ç¬¦ä¸²æ­£ç¡®å¡«å……
                        padding = len(file_content) % 4
                        if padding:
                            file_content += '=' * (4 - padding)
                        
                        decoded_content = base64.b64decode(file_content)
                        
                        # Determine if this is a binary or text file
                        file_ext = safe_filename.lower().split('.')[-1] if '.' in safe_filename else ''
                        is_binary = file_ext in ['pdf', 'docx', 'doc', 'xlsx', 'pptx']
                        
                        if is_binary:
                            with open(upload_path, 'wb') as f:
                                f.write(decoded_content)
                            self.logger.info(f"[Upload] Saved binary file -> {upload_path} ({len(decoded_content)} bytes)")
                        else:
                            with open(upload_path, 'w', encoding='utf-8') as f:
                                f.write(decoded_content.decode('utf-8'))
                            self.logger.info(f"[Upload] Saved text file -> {upload_path} (chars={len(decoded_content.decode('utf-8'))})")
                    except (binascii.Error, UnicodeDecodeError, ValueError) as e:
                        # å¦‚æœä¸æ˜¯base64ï¼Œç›´æ¥ä¿å­˜ä¸ºçº¯æ–‡æœ¬ï¼ˆä½†è®°å½•é”™è¯¯ï¼‰
                        self.logger.warning(f"[Upload] Base64 decode failed for {filename}: {e}")
                        print(f"Content length: {len(file_content)}")
                        print(f"Content sample: {file_content[:100]}...")
                        
                        # å¯¹äºshellå‘½ä»¤ï¼Œè·³è¿‡å¤„ç†
                        if file_content.startswith('$(') and file_content.endswith(')'):
                            print(f"Skipping file {filename} as it contains shell command instead of actual content")
                            continue
                            
                        with open(upload_path, 'w', encoding='utf-8') as f:
                            f.write(file_content)
                        self.logger.info(f"[Upload] Saved raw text file -> {upload_path} (chars={len(file_content)})")
                        
                except Exception as e:
                    self.logger.error(f"[Upload] Error saving file {filename}: {e}")
                    print(f"Content length: {len(file_content)}")
                    print(f"Content sample: {file_content[:100]}...")
                    continue
                
                # è§£ææ–‡æ¡£ç»“æ„
                self.logger.info(f"[Parse] Begin parse -> {upload_path}")
                parsed_doc, created_md = await self._parse_document(upload_path, filename)
                parsed_documents.append(parsed_doc)
                # å¦‚æœäº§å‡ºäº†Markdownæ–‡ä»¶ï¼Œçº³å…¥ files_to_createï¼Œä¾›å‰ç«¯ä¸ä¼šè¯ä¿å­˜
                if created_md and created_md.get("content"):
                    self.logger.info(f"[Parse] MarkItDown produced: {created_md.get('name')} (len={len(created_md.get('content',''))})")
                    files_to_create.append(created_md)
                # æ±‡æ€»ä»Markdownä¸­æŠ½å–çš„å…³é”®ä¿¡æ¯
                if parsed_doc.get("metadata", {}).get("extracted"):
                    aggregated_extracted[filename] = parsed_doc["metadata"]["extracted"]
                
                # ä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                parsed_path = os.path.join(parsed_dir, f"{safe_filename}.json")
                with open(parsed_path, 'w', encoding='utf-8') as f:
                    json.dump(parsed_doc, f, ensure_ascii=False, indent=2)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆäº†wiki/æ‹›æ ‡æ–‡ä»¶.md
            wiki_tender_path = None
            for created_file in files_to_create:
                if created_file.get("name") == "æ‹›æ ‡æ–‡ä»¶.md" and created_file.get("type") == "wiki":
                    wiki_tender_path = "wiki/æ‹›æ ‡æ–‡ä»¶.md"
                    break
            
            response_content = f"æˆåŠŸè§£æäº† {len(parsed_documents)} ä¸ªæ–‡æ¡£ï¼Œè§£æç»“æœå·²ä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹ã€‚"
            if wiki_tender_path:
                response_content += f"\n\nğŸ“„ **è§£æç»“æœ**: å·²ç”Ÿæˆ `{wiki_tender_path}` ç”¨äºåç»­A-Eå·¥ä½œæµã€‚"
            
            self.logger.info(f"[Summary] parsed_docs={len(parsed_documents)}, files_to_create={len(files_to_create)}")
            
            metadata = {
                "parsed_documents": parsed_documents,
                "parsed_files": [f"{doc['filename']}.json" for doc in parsed_documents],
                "files_to_create": files_to_create,
                # ç›´æ¥è¾“å‡ºä¸¤ç±»æ ¸å¿ƒè¦ç´ ï¼Œä¾›åç»­é˜¶æ®µæˆ–UIä½¿ç”¨
                "extracted_info": aggregated_extracted,
                "stage": "document_parsing",
                "action": "parsing_completed"
            }
            
            # å…³é”®ä¿®å¤ï¼šå¦‚æœæˆåŠŸç”Ÿæˆäº†wiki/æ‹›æ ‡æ–‡ä»¶.mdï¼Œæ›´æ–°tender_pathç”¨äºåç»­å·¥ä½œæµ
            if wiki_tender_path:
                metadata["tender_path"] = wiki_tender_path
                metadata["next_stage"] = "bid_build_ready"
            
            return AgentResponse(
                content=response_content,
                metadata=metadata,
                status="completed"
            )
            
        except Exception as e:
            return AgentResponse(
                content=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
                status="error"
            )
    
    async def _parse_document(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æå•ä¸ªæ–‡æ¡£ï¼Œè¿”å›(ç»“æ„åŒ–JSON, ç”Ÿæˆçš„Markdownæ–‡ä»¶ä¿¡æ¯æˆ–None)"""
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            return await self._parse_pdf(file_path, filename)
        elif file_ext in ['.doc', '.docx']:
            return await self._parse_docx(file_path, filename)
        elif file_ext == '.txt':
            return await self._parse_txt(file_path, filename)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    async def _parse_pdf(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æPDFæ–‡æ¡£"""
        try:
            # ä¼˜å…ˆä½¿ç”¨ microsoft/markitdown å°†PDFè½¬ä¸ºMarkdown
            self.logger.info(f"[MarkItDown] Converting PDF -> {file_path}")
            print(f"[MarkItDown] Converting PDF -> {file_path}")
            md_text = self._convert_to_markdown(file_path)
            created_md: Dict[str, Any] = {}
            if md_text:
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                # ç»Ÿä¸€å‘½åä¸º"æ‹›æ ‡æ–‡ä»¶.md"
                base_name = "æ‹›æ ‡æ–‡ä»¶.md"
                md_path = os.path.join(parsed_dir, base_name)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(md_text)
                self.logger.info(f"[MarkItDown] PDF->MD success: {md_path} (chars={len(md_text)})")
                print(f"[MarkItDown] PDF->MD success: {md_path} (chars={len(md_text)})")
                created_md = {"name": base_name, "content": md_text, "type": "wiki", "folder": "parsed"}

                # ä½¿ç”¨LLMåŸºäºMarkdownæŠ½å–ç»“æ„ä¸å…³é”®è¦ç´ 
                structure, extracted = await self._analyze_from_markdown(md_text, filename)
                return ({
                    "filename": filename,
                    "document_type": "PDFæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "markdown_saved": True,
                        "markdown_file": base_name,
                        "extracted": extracted
                    }
                }, created_md)

            # å›é€€ï¼šç›´æ¥æå–æ–‡æœ¬
            import PyPDF2
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = ""
                for page_num, page in enumerate(pdf_reader.pages):
                    text_content += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                    text_content += page.extract_text()
                structure = await self._analyze_document_structure(text_content, filename)
                return ({
                    "filename": filename,
                    "document_type": "PDFæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "total_pages": len(pdf_reader.pages),
                        "file_size": os.path.getsize(file_path),
                        "raw_text": text_content[:5000]
                    }
                }, {})
        except Exception as e:
            raise Exception(f"PDFè§£æå¤±è´¥: {str(e)}")
    
    async def _parse_docx(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æDOCXæ–‡æ¡£"""
        try:
            # ä½¿ç”¨ microsoft/markitdown å°†Wordè½¬ä¸ºMarkdown
            self.logger.info(f"[MarkItDown] Converting DOC/DOCX -> {file_path}")
            print(f"[MarkItDown] Converting DOC/DOCX -> {file_path}")
            md_text = self._convert_to_markdown(file_path)
            created_md: Dict[str, Any] = {}
            if md_text:
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                # æŒ‰éœ€æ±‚ç»Ÿä¸€å‘½åä¸º"æ‹›æ ‡æ–‡ä»¶.md"
                base_name = "æ‹›æ ‡æ–‡ä»¶.md"
                md_path = os.path.join(parsed_dir, base_name)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(md_text)
                self.logger.info(f"[MarkItDown] DOCX->MD success: {md_path} (chars={len(md_text)})")
                print(f"[MarkItDown] DOCX->MD success: {md_path} (chars={len(md_text)})")
                created_md = {"name": base_name, "content": md_text, "type": "wiki", "folder": "parsed"}

                # ä½¿ç”¨LLMåŸºäºMarkdownæŠ½å–ç»“æ„ä¸å…³é”®è¦ç´ 
                structure, extracted = await self._analyze_from_markdown(md_text, filename)
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "markdown_saved": True,
                        "markdown_file": base_name,
                        "extracted": extracted
                    }
                }, created_md)

            # å›é€€ï¼špython-docx æå–çº¯æ–‡æœ¬
            try:
                import docx
                doc = docx.Document(file_path)
                text_content = "\n".join(p.text for p in doc.paragraphs)
                structure = await self._analyze_document_structure(text_content, filename)
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "total_paragraphs": len(doc.paragraphs),
                        "file_size": os.path.getsize(file_path),
                        "raw_text": text_content[:5000]
                    }
                }, {})
            except ImportError:
                self.logger.warning("python-docx not available, creating basic structure")
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": [{
                        "level": 1,
                        "title": "æ–‡æ¡£å†…å®¹",
                        "content": "éœ€è¦å®‰è£…python-docxæ¥è§£æDOCXæ–‡ä»¶",
                        "subsections": []
                    }],
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "parse_method": "fallback"
                    }
                }, {})
        except Exception as e:
            raise Exception(f"DOCXè§£æå¤±è´¥: {str(e)}")
    
    async def _parse_txt(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æTXTæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                text_content = file.read()
            
            # ç®€å•æ–‡æœ¬åˆ†æï¼ˆä¸ä½¿ç”¨LLMï¼‰
            lines = text_content.split('\n')
            sections = []
            current_section = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('## '):
                    # äºŒçº§æ ‡é¢˜
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "level": 2,
                        "title": line[3:],
                        "content": "",
                        "page": "N/A",
                        "subsections": []
                    }
                elif line.startswith('# '):
                    # ä¸€çº§æ ‡é¢˜
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "level": 1,
                        "title": line[2:],
                        "content": "",
                        "page": "N/A",
                        "subsections": []
                    }
                elif current_section and line:
                    # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                    current_section["content"] += line + "\n"
            
            if current_section:
                sections.append(current_section)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç»“æ„
            if not sections:
                sections = [{
                    "level": 1,
                    "title": "æ–‡æ¡£å†…å®¹",
                    "content": text_content[:500] + "..." if len(text_content) > 500 else text_content,
                    "page": "N/A",
                    "subsections": []
                }]
            
            return ({
                "filename": filename,
                "document_type": "TXTæ‹›æ ‡æ–‡ä»¶",
                "structure": sections,
                "metadata": {
                    "file_size": os.path.getsize(file_path),
                    "raw_text": text_content[:1000] + "..." if len(text_content) > 1000 else text_content
                }
            }, {})
        except Exception as e:
            raise Exception(f"TXTè§£æå¤±è´¥: {str(e)}")

    def _convert_to_markdown(self, file_path: str) -> str:
        """ä½¿ç”¨ microsoft/markitdown å°†æ–‡ä»¶è½¬æ¢ä¸ºMarkdownã€‚è‹¥ä¸å¯ç”¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"""
        try:
            from markitdown import MarkItDown  # type: ignore
            md = MarkItDown()
            self.logger.info(f"[MarkItDown] convert() call for: {file_path}")
            print(f"[MarkItDown] convert() call for: {file_path}")
            result = md.convert(file_path)
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„è¿”å›å±æ€§
            if hasattr(result, 'text_content'):
                return getattr(result, 'text_content') or ''
            if hasattr(result, 'markdown'):  # hypothetical
                return getattr(result, 'markdown') or ''
            if isinstance(result, str):
                return result
            # å°è¯•å¸¸è§å±æ€§å
            for key in ('content', 'text'):
                val = getattr(result, key, '')
                if isinstance(val, str) and val:
                    return val
        except Exception as e:
            # é™é»˜å›é€€ï¼Œä»¥ç¡®ä¿ä¸»æµç¨‹ä¸ä¸­æ–­
            self.logger.warning(f"[MarkItDown] convert failed for {file_path}: {e}")
            print(f"[MarkItDown] convert failed for {file_path}: {e}")
        return ""
    
    async def _analyze_from_markdown(self, markdown_text: str, filename: str):
        """åŸºäºå®Œæ•´çš„ Markdown æ–‡æ¡£ï¼š
        - æŠ½å–æ–‡æ¡£ç»“æ„
        - æå–ä¸¤ç±»å…³é”®ä¿¡æ¯ï¼šæŠ•æ ‡æ–‡ä»¶æ ¼å¼ç»“æ„ã€æŠ€æœ¯è§„æ ¼ä¹¦
        è¿”å›: (structure: List[dict], extracted: dict)
        """
        json_schema = """
{
  "structure": [...],
  "extracted": {
    "bid_format": {"found": true/false, "title": "", "content": ""},
    "tech_specifications": {"found": true/false, "title": "", "content": ""}
  }
}
"""

        analysis_prompt = (
            f"ä½ å°†è·å¾—å®Œæ•´çš„æ‹›æ ‡æ–‡ä»¶ï¼ˆå·²è½¬ä¸ºMarkdownï¼‰ã€‚\n"
            f"æ–‡ä»¶åï¼š{filename}\n\n"
            "ä»»åŠ¡ï¼š\n"
            "1) è§£æå¹¶è¿”å›æ–‡æ¡£çš„å±‚çº§ç»“æ„ï¼ˆstructureï¼Œæ•°ç»„ï¼‰ã€‚\n"
            "   - æ¯ä¸ªèŠ‚ç‚¹åŒ…å«: level(1/2/3..), title, contentæ‘˜è¦(<=300å­—), subsections[]\n"
            "2) æå–ä¸¤ç±»å…³é”®ä¿¡æ¯ï¼ˆextracted å¯¹è±¡ï¼‰ï¼š\n"
            "   - bid_format: ä»"æŠ•æ ‡æ–‡ä»¶æ ¼å¼/æŠ•æ ‡æ–‡ä»¶å†…å®¹/å“åº”æ–‡ä»¶æ ¼å¼"ç­‰ç« èŠ‚æå–ç›®å½•ç»“æ„ã€è£…è®¢é¡ºåºã€ç« èŠ‚è¦æ±‚ã€‚\n"
            "   - tech_specifications: ä»"æŠ€æœ¯è§„æ ¼ä¹¦/æŠ€æœ¯è¦æ±‚"ç­‰ç« èŠ‚æå–å…³é”®æŠ€æœ¯æŒ‡æ ‡ä¸çº¦æŸã€‚\n\n"
            "è¯·ä¸¥æ ¼è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼š\n"
            f"{json_schema}\n"
            "å…¨æ–‡Markdownï¼ˆæˆªæ–­å±•ç¤ºï¼Œä¸è¦å›æ˜¾åŸæ–‡ï¼‰ï¼š\n"
            f"{markdown_text[:8000]}\n"
        )
        
        try:
            result_text = await self.llm_client.generate([
                {"role": "system", "content": "ä½ æ˜¯æ–‡æ¡£ç»“æ„ä¸è§„åˆ™æŠ½å–ä¸“å®¶ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ])
            
            # å°è¯•è§£æJSON
            try:
                data = json.loads(result_text)
                structure = data.get('structure', []) if isinstance(data, dict) else []
                extracted = data.get('extracted', {}) if isinstance(data, dict) else {}
                if not isinstance(structure, list):
                    structure = []
                if not isinstance(extracted, dict):
                    extracted = {}
                return structure, extracted
            except:
                pass
            
            # å¤±è´¥å›é€€ï¼šä»…è¿”å›å•èŠ‚ç‚¹ç»“æ„ï¼Œextracted ä¸ºç©º
            return ([{
                "level": 1,
                "title": "æ–‡æ¡£å†…å®¹",
                "content": "ç»“æ„æŠ½å–å¤±è´¥ï¼Œå·²ä¿ç•™åŸæ–‡ç”¨äºåç»­å¤„ç†",
                "subsections": []
            }], {})
            
        except Exception as e:
            # è¿”å›åŸºç¡€ç»“æ„
            return ([{
                "level": 1,
                "title": f"è§£æå¤±è´¥ - {filename}",
                "content": f"ç»“æ„åˆ†æå¤±è´¥: {str(e)}",
                "subsections": []
            }], {})

    async def _analyze_document_structure(self, text_content: str, filename: str):
        """ä½¿ç”¨LLMåˆ†ææ–‡æ¡£ç»“æ„"""
        try:
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶è¯†åˆ«å…¶ç»“æ„ï¼š

æ–‡ä»¶åï¼š{filename}
æ–‡æ¡£å†…å®¹ï¼š
{text_content[:3000]}...

è¯·ä»¥JSONæ ¼å¼è¿”å›æ–‡æ¡£ç»“æ„ï¼ŒåŒ…å«ç« èŠ‚æ ‡é¢˜ã€å±‚çº§å’Œå†…å®¹æ‘˜è¦ã€‚
"""
            
            result_text = await self.llm_client.generate([
                {"role": "system", "content": "ä½ æ˜¯æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ])
            
            # å°è¯•è§£æJSONï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›åŸºç¡€ç»“æ„
            try:
                return json.loads(result_text)
            except:
                return [{
                    "level": 1,
                    "title": "æ–‡æ¡£å†…å®¹",
                    "content": text_content[:300] + "..." if len(text_content) > 300 else text_content,
                    "subsections": []
                }]
        except Exception as e:
            return [{
                "level": 1,
                "title": f"è§£æå¤±è´¥ - {filename}",
                "content": f"ç»“æ„åˆ†æå¤±è´¥: {str(e)}",
                "subsections": []
            }]