"""
æ–‡æ¡£è§£æžæ™ºèƒ½ä½“ - è´Ÿè´£è§£æžä¸Šä¼ çš„æ‹›æ ‡æ–‡ä»¶å¹¶ç”Ÿæˆç»“æž„åŒ–çš„Markdownæ–‡æ¡£
"""
from typing import Dict, Any, List
import os
import json
import binascii
import logging
from pathlib import Path
from .base import BaseAgent, AgentContext, AgentResponse


class DocumentParserAgent(BaseAgent):
    def __init__(self):
        super().__init__("document_parser")
        self.logger = logging.getLogger(__name__)
    
    def get_system_prompt(self) -> str:
        return """æ‚¨æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è§£æžæ™ºèƒ½ä½“ï¼Œä¸“é—¨è´Ÿè´£è§£æžæ‹›æ ‡æ–‡ä»¶ï¼ˆPDF/DOC/DOCXï¼‰å¹¶è¯†åˆ«æ–‡æ¡£ç»“æž„ã€‚

æ‚¨çš„ä¸»è¦èŒè´£ï¼š
1. è¯»å–ä¸Šä¼ çš„æ‹›æ ‡æ–‡ä»¶
2. è¯†åˆ«æ–‡æ¡£ç»“æž„ï¼ˆç« èŠ‚æ ‡é¢˜ã€å†…å®¹ã€é¡µç ç­‰ï¼‰
3. æå–æ–‡æ¡£å…ƒæ•°æ®
4. å°†è§£æžç»“æžœä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹ä¸­

è§£æžè¦æ±‚ï¼š
- å‡†ç¡®è¯†åˆ«ç« èŠ‚æ ‡é¢˜å’Œå±‚çº§å…³ç³»
- æå–æ–‡æ¡£ä¸­çš„è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æž„åŒ–ä¿¡æ¯
- ä¿æŒåŽŸæ–‡æ¡£çš„é€»è¾‘ç»“æž„
- ç”Ÿæˆç»“æž„åŒ–çš„JSONæ ¼å¼è¾“å‡º

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
{
  "filename": "åŽŸæ–‡ä»¶å",
  "document_type": "æ‹›æ ‡æ–‡ä»¶ç±»åž‹",
  "structure": [
    {
      "level": 1,
      "title": "ç« èŠ‚æ ‡é¢˜",
      "content": "ç« èŠ‚å†…å®¹",
      "page": "é¡µç ",
      "subsections": []
    }
  ],
  "metadata": {
    "total_pages": "æ€»é¡µæ•°",
    "creation_date": "åˆ›å»ºæ—¥æœŸ",
    "file_size": "æ–‡ä»¶å¤§å°"
  }
}"""
    
    async def execute(self, context: AgentContext) -> AgentResponse:
        try:
            parsed_documents = []
            files_to_create: List[Dict[str, Any]] = []
            aggregated_extracted: Dict[str, Any] = {}
            
            # === ä¿®å¤5: å‰ç«¯/æ–‡ä»¶è¯»å–è¡¥é½ï¼ˆé¿å…"è§£æž0ä¸ªæ–‡æ¡£"ï¼‰ ===
            # å¦‚æžœå‰ç«¯æ²¡æœ‰ä¼ contentï¼Œå°è¯•ä»ŽåŽç«¯æ–‡ä»¶å­˜å‚¨è¯»å–
            files_to_process = []
            
            for file_info in context.uploaded_files:
                filename = file_info.get('name', 'unknown')
                file_content = file_info.get('content', '')
                file_type = file_info.get('type', '')
                self.logger.info(f"[Upload] Received file: name={filename}, type={file_type}, content_len={len(file_content)}")
                
                # å¦‚æžœæ²¡æœ‰contentï¼Œå°è¯•ä»Ž/mnt/data/è¯»å–
                if not file_content:
                    potential_paths = [
                        f"/mnt/data/{filename}",
                        f"/root/project/git/project-agent/uploads/{filename}",
                        f"./uploads/{filename}"
                    ]
                    
                    for path in potential_paths:
                        if os.path.exists(path):
                            self.logger.info(f"[BackendRead] Found file at: {path}")
                            print(f"[BackendRead] Found file at: {path}")
                            # ç›´æŽ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„
                            files_to_process.append({
                                'name': filename,
                                'path': path,
                                'type': file_type
                            })
                            break
                    else:
                        self.logger.warning(f"[BackendRead] File not found: {filename}")
                        continue
                else:
                    # æœ‰contentï¼ŒæŒ‰åŽŸé€»è¾‘å¤„ç†
                    files_to_process.append({
                        'name': filename,
                        'content': file_content,
                        'type': file_type
                    })
            
            for file_info in files_to_process:
                filename = file_info.get('name', 'unknown')
                file_content = file_info.get('content', '')
                file_path = file_info.get('path', '')
                file_type = file_info.get('type', '')
                
                upload_path = file_path  # å¦‚æžœå·²æœ‰è·¯å¾„ï¼Œç›´æŽ¥ä½¿ç”¨
                safe_filename = ''.join(c for c in filename if c.isalnum() or c in (' ', '.', '-', '_')).rstrip()
                
                if not file_path:  # éœ€è¦ä¿å­˜æ–‡ä»¶å†…å®¹
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°uploadsæ–‡ä»¶å¤¹
                    uploads_dir = "/root/project/git/project-agent/uploads"
                    os.makedirs(uploads_dir, exist_ok=True)
                    upload_path = os.path.join(uploads_dir, safe_filename)
                    
                    # å¤„ç†æ–‡ä»¶å†…å®¹ - æ”¯æŒbase64ç¼–ç å’Œçº¯æ–‡æœ¬
                    import base64
                    try:
                        # é¦–å…ˆå°è¯•è§£ç base64
                        try:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºshellå‘½ä»¤è€ŒéžçœŸå®žbase64å†…å®¹
                            if file_content.startswith('$(') and file_content.endswith(')'):
                                print(f"Warning: File content appears to be a shell command, not base64: {file_content[:50]}...")
                                raise ValueError("File content is a shell command, not actual file content")
                            
                            # ç¡®ä¿base64å­—ç¬¦ä¸²æ­£ç¡®å¡«å……
                            padding = len(file_content) % 4
                            if padding:
                                file_content += '=' * (4 - padding)
                            
                            decoded_content = base64.b64decode(file_content)
                            
                            # Determine if this is a binary or text file
                            file_ext = safe_filename.lower().split('.')[-1] if '.' in safe_filename else ''
                            is_binary = file_ext in ['pdf', 'docx', 'doc', 'xlsx', 'pptx']
                            
                            if is_binary:
                                with open(upload_path, 'wb') as f:
                                    f.write(decoded_content)
                                self.logger.info(f"[Upload] Saved binary file -> {upload_path} ({len(decoded_content)} bytes)")
                            else:
                                with open(upload_path, 'w', encoding='utf-8') as f:
                                    f.write(decoded_content.decode('utf-8'))
                                self.logger.info(f"[Upload] Saved text file -> {upload_path} (chars={len(decoded_content.decode('utf-8'))})")
                        except (binascii.Error, UnicodeDecodeError, ValueError) as e:
                            # å¦‚æžœä¸æ˜¯base64ï¼Œç›´æŽ¥ä¿å­˜ä¸ºçº¯æ–‡æœ¬ï¼ˆä½†è®°å½•é”™è¯¯ï¼‰
                            self.logger.warning(f"[Upload] Base64 decode failed for {filename}: {e}")
                            print(f"Content length: {len(file_content)}")
                            print(f"Content sample: {file_content[:100]}...")
                            
                            # å¯¹äºŽshellå‘½ä»¤ï¼Œè·³è¿‡å¤„ç†
                            if file_content.startswith('$(') and file_content.endswith(')'):
                                print(f"Skipping file {filename} as it contains shell command instead of actual content")
                                continue
                                
                            with open(upload_path, 'w', encoding='utf-8') as f:
                                f.write(file_content)
                            self.logger.info(f"[Upload] Saved raw text file -> {upload_path} (chars={len(file_content)})")
                            
                    except Exception as e:
                        self.logger.error(f"[Upload] Error saving file {filename}: {e}")
                        print(f"Content length: {len(file_content)}")
                        print(f"Content sample: {file_content[:100]}...")
                        continue
                
                # è§£æžæ–‡æ¡£ç»“æž„
                self.logger.info(f"[Parse] Begin parse -> {upload_path}")
                parsed_doc, created_md = await self._parse_document(upload_path, filename)
                parsed_documents.append(parsed_doc)
                
                # å¦‚æžœäº§å‡ºäº†Markdownæ–‡ä»¶ï¼Œçº³å…¥ files_to_createï¼Œä¾›å‰ç«¯ä¸Žä¼šè¯ä¿å­˜
                if created_md and created_md.get("content"):
                    self.logger.info(f"[Parse] MarkItDown produced: {created_md.get('name')} (len={len(created_md.get('content',''))})")
                    files_to_create.append(created_md)
                
                # æ±‡æ€»ä»ŽMarkdownä¸­æŠ½å–çš„å…³é”®ä¿¡æ¯
                if parsed_doc.get("metadata", {}).get("extracted"):
                    aggregated_extracted[filename] = parsed_doc["metadata"]["extracted"]
                
                # ä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                parsed_path = os.path.join(parsed_dir, f"{safe_filename}.json")
                with open(parsed_path, 'w', encoding='utf-8') as f:
                    json.dump(parsed_doc, f, ensure_ascii=False, indent=2)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆäº†Markdownæ–‡ä»¶
            wiki_tender_path = None
            actual_md_path = None
            for created_file in files_to_create:
                if created_file.get("name") == "æ‹›æ ‡æ–‡ä»¶.md" and created_file.get("type") == "wiki":
                    # âœ… åªä¿¡ MarkItDown è¿”å›žçš„çœŸå®žè·¯å¾„ï¼Œåˆ«å†æ‹¼ backend/wiki/...
                    wiki_tender_path = "wiki/æ‹›æ ‡æ–‡ä»¶.md"  # ç”¨äºŽæ˜¾ç¤º
                    # å®žé™…ä½¿ç”¨è§£æžå™¨äº§å‡ºçš„è·¯å¾„ - åœ¨ _parse_* æ–¹æ³•ä¸­å·²ç»å†™å…¥äº†æ­£ç¡®è·¯å¾„
                    from pathlib import Path
                    # ä»Ž MarkItDown å®žé™…äº§å‡ºçš„è·¯å¾„èŽ·å–
                    parsed_dir = "/root/project/git/project-agent/wiki"
                    actual_md_path = Path(parsed_dir) / "æ‹›æ ‡æ–‡ä»¶.md"
                    if actual_md_path.exists():
                        actual_md_path = actual_md_path.resolve()
                    break
            
            response_content = f"æˆåŠŸè§£æžäº† {len(parsed_documents)} ä¸ªæ–‡æ¡£ï¼Œè§£æžç»“æžœå·²ä¿å­˜åˆ°wikiæ–‡ä»¶å¤¹ã€‚"
            if wiki_tender_path:
                response_content += f"\n\nðŸ“„ **è§£æžç»“æžœ**: å·²ç”Ÿæˆ `{wiki_tender_path}` ç”¨äºŽåŽç»­A-Eå·¥ä½œæµã€‚"
            
            self.logger.info(f"[Summary] parsed_docs={len(parsed_documents)}, files_to_create={len(files_to_create)}")
            
            metadata = {
                "parsed_documents": parsed_documents,
                "parsed_files": [f"{doc['filename']}.json" for doc in parsed_documents],
                "files_to_create": files_to_create,
                # ç›´æŽ¥è¾“å‡ºä¸¤ç±»æ ¸å¿ƒè¦ç´ ï¼Œä¾›åŽç»­é˜¶æ®µæˆ–UIä½¿ç”¨
                "extracted_info": aggregated_extracted,
                "stage": "parsing_completed",  # ä¿®å¤ï¼šä½¿ç”¨æ˜Žç¡®çš„å®ŒæˆçŠ¶æ€
                "action": "parsing_completed"
            }
            
            # === ä¿®å¤3: è§£æžå™¨æ”¶å°¾ï¼šè½é”š"å·²è§£æž"ï¼ŒæŽ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ ===
            if actual_md_path and actual_md_path.exists():
                # âœ… actual_md_path æ˜¯ MarkItDown å®žé™…ç”Ÿæˆçš„æ–‡ä»¶ï¼Œå·²ç»æ˜¯ç»å¯¹è·¯å¾„
                abs_path = str(actual_md_path)
                metadata["tender_path"] = abs_path  # ç»å¯¹è·¯å¾„
                metadata.setdefault("parsed_files", []).append(abs_path)
                
                # å¯é€‰ï¼šæŠŠå†…å®¹æ”¾è¿›å†…å­˜ï¼ŒåŽç»­ Aâ€“E ä¸å¿…å†ç¢°æ–‡ä»¶ç³»ç»Ÿ
                try:
                    metadata["tender_md"] = actual_md_path.read_text(encoding="utf-8")
                    print(f"[Parser] Loaded tender content: {len(metadata['tender_md'])} chars")
                except Exception as e:
                    print(f"[Parser] read_text failed (non-fatal): {e}")
                
                # æ˜Žç¡®æŽ¨è¿›
                metadata["current_stage"] = "parsing_completed"
                metadata["next_stage"] = "bid_build_ready"
                
                # å…³é”®çŠ¶æ€æ ‡è®°ï¼šé˜²æ­¢å†æ¬¡è‡ªåŠ¨è§¦å‘è§£æž
                metadata["files_parsed"] = True
                metadata["parsing_auto_triggered"] = True
            
            return AgentResponse(
                content=response_content,
                metadata=metadata,
                status="completed"
            )
            
        except Exception as e:
            return AgentResponse(
                content=f"æ–‡æ¡£è§£æžå¤±è´¥: {str(e)}",
                status="error"
            )
    
    async def _parse_document(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æžå•ä¸ªæ–‡æ¡£ï¼Œè¿”å›ž(ç»“æž„åŒ–JSON, ç”Ÿæˆçš„Markdownæ–‡ä»¶ä¿¡æ¯æˆ–None)"""
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            return await self._parse_pdf(file_path, filename)
        elif file_ext in ['.doc', '.docx']:
            return await self._parse_docx(file_path, filename)
        elif file_ext == '.txt':
            return await self._parse_txt(file_path, filename)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    async def _parse_pdf(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æžPDFæ–‡æ¡£"""
        try:
            # ä¼˜å…ˆä½¿ç”¨ microsoft/markitdown å°†PDFè½¬ä¸ºMarkdown
            self.logger.info(f"[MarkItDown] Converting PDF -> {file_path}")
            print(f"[MarkItDown] Converting PDF -> {file_path}")
            md_text = self._convert_to_markdown(file_path)
            created_md: Dict[str, Any] = {}
            if md_text:
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                # ç»Ÿä¸€å‘½åä¸º"æ‹›æ ‡æ–‡ä»¶.md"
                base_name = "æ‹›æ ‡æ–‡ä»¶.md"
                md_path = os.path.join(parsed_dir, base_name)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(md_text)
                self.logger.info(f"[MarkItDown] PDF->MD success: {md_path} (chars={len(md_text)})")
                print(f"[MarkItDown] PDF->MD success: {md_path} (chars={len(md_text)})")
                created_md = {"name": base_name, "content": md_text, "type": "wiki", "folder": "parsed"}

                # ä½¿ç”¨LLMåŸºäºŽMarkdownæŠ½å–ç»“æž„ä¸Žå…³é”®è¦ç´ 
                structure, extracted = await self._analyze_from_markdown(md_text, filename)
                return ({
                    "filename": filename,
                    "document_type": "PDFæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "markdown_saved": True,
                        "markdown_file": base_name,
                        "extracted": extracted
                    }
                }, created_md)

            # å›žé€€ï¼šç›´æŽ¥æå–æ–‡æœ¬
            import PyPDF2
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = ""
                for page_num, page in enumerate(pdf_reader.pages):
                    text_content += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                    text_content += page.extract_text()
                structure = await self._analyze_document_structure(text_content, filename)
                return ({
                    "filename": filename,
                    "document_type": "PDFæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "total_pages": len(pdf_reader.pages),
                        "file_size": os.path.getsize(file_path),
                        "raw_text": text_content[:5000]
                    }
                }, {})
        except Exception as e:
            raise Exception(f"PDFè§£æžå¤±è´¥: {str(e)}")
    
    async def _parse_docx(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æžDOCXæ–‡æ¡£"""
        try:
            # ä½¿ç”¨ microsoft/markitdown å°†Wordè½¬ä¸ºMarkdown
            self.logger.info(f"[MarkItDown] Converting DOC/DOCX -> {file_path}")
            print(f"[MarkItDown] Converting DOC/DOCX -> {file_path}")
            md_text = self._convert_to_markdown(file_path)
            created_md: Dict[str, Any] = {}
            if md_text:
                parsed_dir = "/root/project/git/project-agent/wiki"
                os.makedirs(parsed_dir, exist_ok=True)
                # æŒ‰éœ€æ±‚ç»Ÿä¸€å‘½åä¸º"æ‹›æ ‡æ–‡ä»¶.md"
                base_name = "æ‹›æ ‡æ–‡ä»¶.md"
                md_path = os.path.join(parsed_dir, base_name)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(md_text)
                self.logger.info(f"[MarkItDown] DOCX->MD success: {md_path} (chars={len(md_text)})")
                print(f"[MarkItDown] DOCX->MD success: {md_path} (chars={len(md_text)})")
                created_md = {"name": base_name, "content": md_text, "type": "wiki", "folder": "parsed"}

                # ä½¿ç”¨LLMåŸºäºŽMarkdownæŠ½å–ç»“æž„ä¸Žå…³é”®è¦ç´ 
                structure, extracted = await self._analyze_from_markdown(md_text, filename)
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "markdown_saved": True,
                        "markdown_file": base_name,
                        "extracted": extracted
                    }
                }, created_md)

            # å›žé€€ï¼špython-docx æå–çº¯æ–‡æœ¬
            try:
                import docx
                doc = docx.Document(file_path)
                text_content = "\n".join(p.text for p in doc.paragraphs)
                structure = await self._analyze_document_structure(text_content, filename)
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": structure,
                    "metadata": {
                        "total_paragraphs": len(doc.paragraphs),
                        "file_size": os.path.getsize(file_path),
                        "raw_text": text_content[:5000]
                    }
                }, {})
            except ImportError:
                self.logger.warning("python-docx not available, creating basic structure")
                return ({
                    "filename": filename,
                    "document_type": "DOCXæ‹›æ ‡æ–‡ä»¶",
                    "structure": [{
                        "level": 1,
                        "title": "æ–‡æ¡£å†…å®¹",
                        "content": "éœ€è¦å®‰è£…python-docxæ¥è§£æžDOCXæ–‡ä»¶",
                        "subsections": []
                    }],
                    "metadata": {
                        "file_size": os.path.getsize(file_path),
                        "parse_method": "fallback"
                    }
                }, {})
        except Exception as e:
            raise Exception(f"DOCXè§£æžå¤±è´¥: {str(e)}")
    
    async def _parse_txt(self, file_path: str, filename: str) -> tuple[Dict[str, Any], Dict[str, Any]]:
        """è§£æžTXTæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                text_content = file.read()
            
            # ç®€å•æ–‡æœ¬åˆ†æžï¼ˆä¸ä½¿ç”¨LLMï¼‰
            lines = text_content.split('\n')
            sections = []
            current_section = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('## '):
                    # äºŒçº§æ ‡é¢˜
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "level": 2,
                        "title": line[3:],
                        "content": "",
                        "page": "N/A",
                        "subsections": []
                    }
                elif line.startswith('# '):
                    # ä¸€çº§æ ‡é¢˜
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "level": 1,
                        "title": line[2:],
                        "content": "",
                        "page": "N/A",
                        "subsections": []
                    }
                elif current_section and line:
                    # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                    current_section["content"] += line + "\n"
            
            if current_section:
                sections.append(current_section)
            
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç»“æž„
            if not sections:
                sections = [{
                    "level": 1,
                    "title": "æ–‡æ¡£å†…å®¹",
                    "content": text_content[:500] + "..." if len(text_content) > 500 else text_content,
                    "page": "N/A",
                    "subsections": []
                }]
            
            return ({
                "filename": filename,
                "document_type": "TXTæ‹›æ ‡æ–‡ä»¶",
                "structure": sections,
                "metadata": {
                    "file_size": os.path.getsize(file_path),
                    "raw_text": text_content[:1000] + "..." if len(text_content) > 1000 else text_content
                }
            }, {})
        except Exception as e:
            raise Exception(f"TXTè§£æžå¤±è´¥: {str(e)}")

    def _convert_to_markdown(self, file_path: str) -> str:
        """ä½¿ç”¨ microsoft/markitdown å°†æ–‡ä»¶è½¬æ¢ä¸ºMarkdownã€‚è‹¥ä¸å¯ç”¨åˆ™è¿”å›žç©ºå­—ç¬¦ä¸²ã€‚"""
        try:
            from markitdown import MarkItDown  # type: ignore
            md = MarkItDown()
            self.logger.info(f"[MarkItDown] convert() call for: {file_path}")
            print(f"[MarkItDown] convert() call for: {file_path}")
            result = md.convert(file_path)
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„è¿”å›žå±žæ€§
            if hasattr(result, 'text_content'):
                return getattr(result, 'text_content') or ''
            if hasattr(result, 'markdown'):  # hypothetical
                return getattr(result, 'markdown') or ''
            if isinstance(result, str):
                return result
            # å°è¯•å¸¸è§å±žæ€§å
            for key in ('content', 'text'):
                val = getattr(result, key, '')
                if isinstance(val, str) and val:
                    return val
        except Exception as e:
            # é™é»˜å›žé€€ï¼Œä»¥ç¡®ä¿ä¸»æµç¨‹ä¸ä¸­æ–­
            self.logger.warning(f"[MarkItDown] convert failed for {file_path}: {e}")
            print(f"[MarkItDown] convert failed for {file_path}: {e}")
        return ""
    
    async def _analyze_from_markdown(self, markdown_text: str, filename: str):
        """åŸºäºŽå®Œæ•´çš„ Markdown æ–‡æ¡£æŠ½å–æ–‡æ¡£ç»“æž„å’Œå…³é”®ä¿¡æ¯"""
        # ç®€åŒ–å®žçŽ°ï¼Œé¿å…å¤æ‚çš„LLMè°ƒç”¨
        structure = [{
            "level": 1,
            "title": "æ‹›æ ‡æ–‡ä»¶å†…å®¹",
            "content": markdown_text[:300] + "..." if len(markdown_text) > 300 else markdown_text,
            "subsections": []
        }]
        
        extracted = {
            "bid_format": {"found": False, "title": "", "content": ""},
            "tech_specifications": {"found": False, "title": "", "content": ""}
        }
        
        return structure, extracted

    async def _analyze_document_structure(self, text_content: str, filename: str):
        """ç®€åŒ–çš„æ–‡æ¡£ç»“æž„åˆ†æž"""
        return [{
            "level": 1,
            "title": "æ–‡æ¡£å†…å®¹",
            "content": text_content[:300] + "..." if len(text_content) > 300 else text_content,
            "subsections": []
        }]